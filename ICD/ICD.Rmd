---
title: "Indice de Calidad - PERU"
author: "David Dominguez - A01570975"
date: "2023-09-14"
output: html_document
---

# Llamar Librerias
```{r message=FALSE, warning=FALSE}
library(dplyr)
library(readr)
library(readxl)
library(tidyverse)
library(fs) # para funciones de sistema de archivos
library(purrr)
library(openxlsx)
library(sf)
library(httr)
library(jsonlite)
library(ggplot2)
```

# Carga de Base de Datos

## Quality Data
```{r message=FALSE, warning=FALSE}
setwd("fuentes_datos/")  # Establecer ruta de trabajo

# Función para leer y combinar archivos de una carpeta
combine_files <- function(path, file_type = c("csv", "excel")) {
  
  # Obtener lista de archivos en el directorio especificado
  files <- list.files(path, full.names = TRUE)
  
  df_combined <- data.frame()
  
  process_file <- function(file) {
  tryCatch({
    if (grepl("\\.csv$", file)) {
      data <- read_csv(file)
    } else if (grepl("\\.(xlsx|xls|XLSX|XLS)$", file)) {
      data <- read_excel(file)
    } else {
      stop("Tipo de archivo no soportado")
    }
    
    # Lista de columnas que deben ser double
    cols_to_double <- c("Duration(Sec)", "longitude", "latitude", "SessionEndLongitude", "SessionEndLatitude")
    
    # Convertir todas las columnas a character, excepto las especificadas
    data[] <- lapply(names(data), function(col) {
      if (col %in% cols_to_double) {
        as.numeric(as.character(data[[col]]))
      } else {
        as.character(data[[col]])
      }
    })
    
    return(data)
  }, error = function(e) {
    print(paste("Error al procesar el archivo:", file))  # Imprimir el archivo con error
    print(e)  # Imprimir el error específico
    return(NULL)
  })
}

  if ("csv" %in% file_type) {
    files_csv <- grep("\\.csv$", files, value = TRUE)
    if(length(files_csv) > 0){
      df_list_csv <- lapply(files_csv, process_file)
      df_combined <- bind_rows(df_combined, do.call(bind_rows, df_list_csv))
    }
  }
  
  if ("excel" %in% file_type) {
    files_xls <- grep("\\.(xlsx|xls|XLSX|XLS)$", files, value = TRUE)
    if(length(files_xls) > 0){
      df_list_xls <- lapply(files_xls, process_file)
      df_combined <- bind_rows(df_combined, do.call(bind_rows, df_list_xls))
    }
  }
  
  # Eliminar filas duplicadas
  if(nrow(df_combined) > 0) {
    df_combined <- df_combined %>% distinct()
  }
  
  return(df_combined)
}

# Crear data frames para cada carpeta
df_actual <- combine_files("actual/")
df_session <- combine_files("session/")
df_survey <- combine_files("survey/")
df_scenes <- combine_files("scenes/")
```

## Master Clientes
```{r message=FALSE, warning=FALSE}
# Leer todos los archivos en fuentes_datos/clientes, excepto el de clientes que leeremos directamente
all_dfs <- read_excel("fuentes_datos/clientes/MD TRAD PERU 11.23.xlsx")

# Función para limpiar y estandarizar nombres de columnas
limpiar_nombres <- function(nombres) {
  nombres <- tolower(nombres)
  nombres <- chartr("áéíóú", "aeiou", nombres)
  nombres <- gsub(" ", "_", nombres)
  return(nombres)
}

# Extraer nombres de columnas, limpiarlos y reasignarlos
nombres_columnas <- names(all_dfs)
nombres_columnas_limpios <- limpiar_nombres(nombres_columnas)
names(all_dfs) <- nombres_columnas_limpios

# Eliminar duplicados, manteniendo el último
all_dfs <- all_dfs %>%
  arrange(desc(codigo_cliente)) %>% 
  distinct(codigo_cliente, .keep_all = TRUE)

# Hacer el left_join con clientes
master_clientes <- all_dfs 

# Verificar el resultado
head(master_clientes)
```

## Parametros
```{r message=FALSE, warning=FALSE}
# Cargar la hoja "data" del archivo "parametros"
parametros_data <- read_excel("parametros.xlsx", sheet = "data")

# Verificar la carga de datos
head(parametros_data)
```

# Sabana de Analisis

## Tiempo
```{r}
df_survey <- df_survey %>%
  mutate(duration = `Duration(Sec)` / 60) %>%
  mutate(estatus = if_else(Status == "Complete", 1, 0))

# Verificar los cambios
head(df_survey)
```

## Coordenadas
```{r}
# Función del haversine para calcular distancia entre dos puntos de latitud y longitud
haversine <- function(lon1, lat1, lon2, lat2) {
  R <- 6371000  # Radio de la Tierra en metros
  phi1 <- lat1 * (pi / 180)
  phi2 <- lat2 * (pi / 180)
  delta_phi <- (lat2 - lat1) * (pi / 180)
  delta_lambda <- (lon2 - lon1) * (pi / 180)
  
  a <- sin(delta_phi / 2)^2 + cos(phi1) * cos(phi2) * sin(delta_lambda / 2)^2
  c <- 2 * atan2(sqrt(a), sqrt(1 - a))
  
  d <- R * c  # Distancia en metros
  return(d)
}

# Convertir las columnas relevantes a character
df_session$OutletCode <- as.character(df_session$OutletCode)
master_clientes$codigo_cliente <- as.character(master_clientes$codigo_cliente)

# Realizar el join y calcular la distancia usando la función haversine
df_session <- df_session %>%
  left_join(select(master_clientes, codigo_cliente, latitude, longitude),
            by = c("OutletCode" = "codigo_cliente")) %>%
  mutate(
    distance = mapply(haversine, longitude, latitude, SessionEndLongitude, SessionEndLatitude)
  )

# Calculamos los cuartiles y el rango intercuartil de la distancia
Q1 <- quantile(df_session$distance, 0.25, na.rm = TRUE)
Q3 <- quantile(df_session$distance, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1

# Reemplazar valores atípicos por 0
df_session$distance[df_session$distance < (Q1 - 1.5 * IQR) | 
                    df_session$distance > (Q3 + 1.5 * IQR)] <- 0

# Identificar valores atípicos
outliers <- df_session$distance[df_session$distance < (Q1 - 1.5 * IQR) | 
                                df_session$distance > (Q3 + 1.5 * IQR)]

# Imprimir valores atípicos
print(outliers)


# Verificar los cambios
head(df_session)
```

## Frentes
```{r}
frentes_df <- df_actual %>%
  group_by(SessionUID) %>%
  summarise(
    # Cálculo de num_frentes
    num_frentes = sum(ProductName != "Foreign" & IsEmpty == FALSE),
    # Cálculo de frentes_com
    frentes_com = sum(ProductName != "Foreign" & IsForeign == TRUE),
    # Cálculo de frentes_total
    frentes_total = sum(ProductName != "Foreign" & IsEmpty == FALSE & BeverageType != "Lacteos")
  ) %>%
  mutate(
    # Cálculo de frentes_arca
    frentes_arca = frentes_total - frentes_com,
    # Cálculo de sovi
    sovi = ifelse(frentes_total == 0, 0, frentes_arca / frentes_total)
  )

# Verificar el nuevo dataframe
head(frentes_df)
```

## Enfriadores
```{r}
result_enfriadores <- df_scenes %>%
  # Agrupar por SessionUID y SubSceneType y contar
  group_by(SessionUID, SubSceneType) %>%
  count() %>%
  # Transforma valores de SubSceneType en columnas individuales
  spread(key = SubSceneType, value = n, fill = 0) %>% # fill = 0 para llenar con 0s donde no haya conteos
  ungroup()

# Funcion para sumar Enfriadores 

# Genera las columnas si no existen
if (!"EDF ACL" %in% names(result_enfriadores)) {
  result_enfriadores$`EDF ACL` <- 0
}

# Suma las columnas
result_enfriadores <- result_enfriadores %>%
  mutate(enfriador_total = `EDF ACL`)

# Verificar el resultado
head(result_enfriadores)
```

## Scenes
```{r}
# Calculando el número total de escenas por SessionUID
df_total_scenes <- df_scenes %>%
  group_by(SessionUID) %>%
  summarise(Num.Scenes = n(), .groups = "drop")

# Calculando el número de escenas de tipo 'Ambiente'
df_ambiente <- df_scenes %>%
  filter(SceneType == "Ambiente") %>%
  group_by(SessionUID) %>%
  summarise(Scenes_Amb = n(), .groups = "drop")

# Calculando el número de escenas de tipo 'Frio'
df_frio <- df_scenes %>%
  filter(SceneType == "Frio") %>%
  group_by(SessionUID) %>%
  summarise(Scenes_Frio = n(), .groups = "drop")

# Uniendo los dataframes
df_scenenum <- df_total_scenes %>%
  left_join(df_ambiente, by = "SessionUID") %>%
  left_join(df_frio, by = "SessionUID")

# Reemplazando NA con 0 para las columnas Scenes_Amb y Scenes_Frio
df_scenenum$Scenes_Amb[is.na(df_scenenum$Scenes_Amb)] <- 0
df_scenenum$Scenes_Frio[is.na(df_scenenum$Scenes_Frio)] <- 0

# Mostrando el resultado final
print(df_scenenum)
```

## Flags
```{r}
# Creación del vector de valores de flag
flag_values <- c(2, 4, 13, 14, 15, 16, 26, 27, 32, 36, 39, 31)
#-------------------------------------------------------------

# Función para dividir la cadena en pares y añadir comas
format_image_quality <- function(value) {
  # Divide la cadena en pares de caracteres
  split_values <- str_extract_all(value, ".{1,2}")[[1]]
  # Une los valores con comas
  paste(split_values, collapse = ",")
}

# Aplica la función a la columna ImageQuality
df_scenes$ImageQuality <- sapply(df_scenes$ImageQuality, format_image_quality)

# Ahora procede con la agrupación y concatenación
result_scenes <- df_scenes %>%
  group_by(SessionUID) %>%
  summarise(ImageQuality = paste(unique(ImageQuality), collapse = ",")) %>%
  ungroup()

#--------------------------------------------------------------
# Función para detectar las flags en la columna ImageQuality
detected_flags <- function(quality_string) {
  quality_values <- as.numeric(unlist(strsplit(quality_string, ",")))  # Divide la cadena y convierte a numéricos
  detected <- quality_values[quality_values %in% flag_values]
  if(length(detected) > 0) {
    return(paste(detected, collapse = ","))
  } else {
    return(NA_character_)
  }
}

# Función modificada para contar las flags y asignar -10 por cada flag presente
check_flags <- function(quality_string) {
  quality_values <- as.numeric(unlist(strsplit(quality_string, ",")))  # Divide la cadena y convierte a numéricos
  flag_count <- sum(quality_values %in% flag_values)  # Cuenta cuántas flags están presentes
  return(100 + (-10 * flag_count))  # Asigna -10 por cada flag
}

# Función para detectar las flags en la columna ImageQuality (ya existente en tu código)
detect_flags <- function(quality_string) {
  quality_values <- as.numeric(unlist(strsplit(quality_string, ",")))  # Divide la cadena y convierte a numéricos
  detected <- quality_values[quality_values %in% flag_values]
  if(length(detected) > 0) {
    return(paste(detected, collapse = ","))
  } else {
    return(NA_character_)
  }
}

# Aplicar ambas funciones a la columna ImageQuality de result_scenes
result_scenes$flag_trigger <- sapply(result_scenes$ImageQuality, check_flags)
result_scenes$detected_flags <- sapply(result_scenes$ImageQuality, detect_flags)

# Verificar el resultado
head(result_scenes)
```

## Cuotas



# Sabana Calidad Semilla 
```{r}
master_calidad <- df_survey %>%
  select(SessionUID = "Session Uid", SurveyType = "Survey Type", User, `Outlet Code`, duration, `Survey End Time`, estatus) %>%
  
  # Unión con master_clientes
  left_join(master_clientes %>% 
              mutate(codigo_cliente = as.character(codigo_cliente)) %>%
              select(codigo_cliente, tamaño_homologado, region_comercial, canal_isscom, subcanal_isscom, subregion_comercial, territorio),
            by = c("Outlet Code" = "codigo_cliente")) %>%
  
  # Incorporar columnas de df_session
  left_join(df_session %>% select(SessionUId, distance),
            by = c("SessionUID" = "SessionUId")) %>%  
  
  # Incorporar métricas de frentes_df
  left_join(frentes_df %>% select(SessionUID, frentes_total, frentes_arca, sovi),
            by = "SessionUID") %>%

  # Incorporar columna flag_trigger de result_scenes
  left_join(result_scenes %>% select(SessionUID, flag_trigger, detected_flags),
            by = "SessionUID") %>%

  # Incorporar columna enfriador_total de result_enfriadores
  left_join(result_enfriadores %>% select(SessionUID, enfriador_total),
            by = "SessionUID") %>%
  
  # Seleccionar y reordenar columnas
  select(
    SessionUID, SurveyType, User, `Outlet Code`, 
    region_comercial, tamaño_homologado, canal_isscom, subcanal_isscom,
    duration, distance,
    frentes_total, frentes_arca, sovi, flag_trigger, enfriador_total, detected_flags, subregion_comercial, territorio, `Survey End Time`, estatus
  )

# Añadir la información de df_scenenum a master_calidad
master_calidad <- master_calidad %>%
  left_join(df_scenenum, by = "SessionUID")

# Verificar el nuevo dataframe
head(master_calidad)
```

## Eliminar Directorio
```{r}
# Función para limpiar directorio de archivos individuales, manteniendo solo el combinado
clean_directory <- function(path) {
  combined_file_path <- file.path(path, "combined.csv")
  
  # Listar todos los archivos excepto el archivo combinado
  files_to_delete <- setdiff(list.files(path, full.names = TRUE), combined_file_path)
  
  # Eliminar archivos
  file.remove(files_to_delete)
}

# Limpiar directorios de archivos individuales
clean_directory("fuentes_datos/session/")
clean_directory("fuentes_datos/survey/")
```

# Correción de Datos

## Flags Inducidas 
```{r}
# 1. Contar los SceneUID distintos para cada SessionUID
scene_count_df <- df_actual %>%
  group_by(SessionUID) %>%
  summarise(min_frentes = n_distinct(SceneUID) * 3) 

# Unir temporalmente con master_calidad para aplicar las condiciones
master_calidad_temp <- left_join(master_calidad, scene_count_df, by = "SessionUID")

# Actualizar las flags de acuerdo con las condiciones
master_calidad <- master_calidad_temp %>%
  mutate(
    # Flag para 0 frentes totales
    detected_flags = ifelse(frentes_total == 0, 
                            ifelse(is.na(detected_flags), "66", paste(detected_flags, ",66", sep="")),
                            detected_flags),
    flag_trigger = ifelse(frentes_total == 0, -100, flag_trigger),
    # Flag para frentes insuficientes
    detected_flags = ifelse(frentes_total < min_frentes, 
                            ifelse(is.na(detected_flags), "61", paste(detected_flags, ",61", sep="")),
                            detected_flags),
    flag_trigger = ifelse(frentes_total < min_frentes, -100, flag_trigger),
    # Nueva flag para enfriador_total no nulo y frentes_total nulo
    detected_flags = ifelse(!is.na(enfriador_total) & is.na(frentes_total),
                            ifelse(is.na(detected_flags), "-100", paste(detected_flags, ",-100", sep="")),
                            detected_flags),
    flag_trigger = ifelse(!is.na(enfriador_total) & is.na(frentes_total), -100, flag_trigger)
  ) %>%
  select(-min_frentes)  # Eliminamos la columna temporal min_frentes

head(master_calidad)
```

## Session End Time (as.Date and only first 10)
```{r}
master_calidad <- master_calidad %>%
  mutate(
    `Survey End Time` = as.Date(substr(`Survey End Time`, 1, 10), format = "%d/%m/%Y")
  )

# Corregir la columna 'detected_flags' para asegurar que haya comas entre cada dos caracteres
master_calidad$detected_flags <- str_replace_all(master_calidad$detected_flags, "(\\d{2})(?!$)", "\\1,")

# Verificar los primeros registros para asegurar que la corrección se aplicó correctamente
head(master_calidad)
```

----------EVALUACIÓN-----------
# Evaluación de Data
```{r}
# Paso 1: Unión con parametros_data
master_evaluado <- master_calidad %>%
  left_join(parametros_data, by = c("tamaño_homologado" = "tamaño_homologado", "canal_isscom", "subcanal_isscom"))

# Paso 2: Generar subconjuntos basados en combinaciones de tradechannelcode, tamaño y sub_canal_isscom "General"
# Asegúrate de que cada subconjunto tenga exactamente una fila
tradicional_CH <- parametros_data[parametros_data$canal_isscom == "VIV.LOCALES TRADICIONALES" & 
                                  parametros_data$tamaño_homologado == "Chico" & 
                                  parametros_data$subcanal_isscom == "General", ][1, ]
tradicional_M  <- parametros_data[parametros_data$canal_isscom == "VIV.LOCALES TRADICIONALES" & 
                                  parametros_data$tamaño_homologado == "Mediano"  & 
                                  parametros_data$subcanal_isscom == "General", ][1, ]
tradicional_G  <- parametros_data[parametros_data$canal_isscom == "VIV.LOCALES TRADICIONALES" & 
                                  parametros_data$tamaño_homologado == "Grande"  & 
                                  parametros_data$subcanal_isscom == "General", ][1, ]

# Replicar los valores de cada subconjunto para que coincidan con el número de filas en master_evaluado
n_rows <- nrow(master_evaluado)
tradicional_CH_rep <- tradicional_CH[rep(1, n_rows), ]
tradicional_M_rep  <- tradicional_M[rep(1, n_rows), ]
tradicional_G_rep  <- tradicional_G[rep(1, n_rows), ]

# Paso 3: Usar bucle para iterar sobre columnas y reemplazar NAs
cols_to_modify <- c("lower_bound_frentes", "upper_bound_frentes", 
                    "lower_bound_frentes_arca", "upper_bound_frentes_arca", 
                    "lower_bound_enfriadores", "upper_bound_enfriadores", 
                    "lower_bound_duration", "upper_bound_duration",
                    "lower_bound_NumScenes", "upper_bound_NumScenes",
                    "lower_bound_ScenesAmb", "upper_bound_ScenesAmb",
                    "lower_bound_ScenesFrio", "upper_bound_ScenesFrio")

# Asegúrate de que cada subconjunto tenga al menos una fila de datos
if (nrow(tradicional_CH) == 0 || nrow(tradicional_M) == 0 || nrow(tradicional_G) == 0) {
  stop("Uno de los subconjuntos está vacío")
}

# Iterar sobre las columnas para reemplazar NAs
for(col in cols_to_modify) {
  # Tradicional CH
  master_evaluado[[col]][is.na(master_evaluado[[col]]) & 
                         master_evaluado$canal_isscom == "VIV.LOCALES TRADICIONALES" & 
                         master_evaluado$tamaño_homologado == "Chico"] <- tradicional_CH[[col]][1]
  
  # Tradicional M
  master_evaluado[[col]][is.na(master_evaluado[[col]]) & 
                         master_evaluado$canal_isscom == "VIV.LOCALES TRADICIONALES" & 
                         master_evaluado$tamaño_homologado == "Mediano"] <- tradicional_M[[col]][1]
  
  # Tradicional G
  master_evaluado[[col]][is.na(master_evaluado[[col]]) & 
                         master_evaluado$canal_isscom == "VIV.LOCALES TRADICIONALES" & 
                         master_evaluado$tamaño_homologado == "Grande"] <- tradicional_G[[col]][1]
  
  # NA en tradechannelcode
  master_evaluado[[col]][is.na(master_evaluado[[col]]) & 
                         is.na(master_evaluado$canal_isscom) & 
                         master_evaluado$tamaño_homologado == "Mediano"] <- tradicional_M[[col]][1]
  
  # Tamaño es NA pero tradechannelcode es "Tradicional"
  master_evaluado[[col]][is.na(master_evaluado[[col]]) & 
                         is.na(master_evaluado$tamaño_homologado) & 
                         master_evaluado$canal_isscom == "VIV.LOCALES TRADICIONALES"] <- tradicional_M[[col]][1]
}

# Paso 4
# Imputación final para reemplazar cualquier NA restante con los valores de 'tradicional_M'
for(col in cols_to_modify) {
  if(nrow(tradicional_M) > 0) {
    master_evaluado[[col]][is.na(master_evaluado[[col]])] <- tradicional_M[[col]][1]
  } else {
    stop("El subconjunto tradicional_M está vacío o no existe.")
  }
}
# Paso 5: Calificación
master_evaluado <- master_evaluado %>%
  mutate(
    # Calificación para frentes_total
    score_frentes_total = case_when(
      frentes_total >= lower_bound_frentes & frentes_total <= upper_bound_frentes ~ 100,
      frentes_total >= (lower_bound_frentes - 0.01 * (upper_bound_frentes - lower_bound_frentes)) &
        frentes_total <= (upper_bound_frentes + 0.01 * (upper_bound_frentes - lower_bound_frentes)) ~ 50,
      TRUE ~ 0
    ),
    # Calificación para frentes_total
    score_frentes_arca = case_when(
      frentes_arca >= lower_bound_frentes_arca & frentes_arca <= upper_bound_frentes_arca ~ 100,
      frentes_arca >= (lower_bound_frentes_arca - 0.01 * (upper_bound_frentes_arca - lower_bound_frentes_arca)) &
        frentes_arca <= (upper_bound_frentes_arca + 0.01 * (upper_bound_frentes_arca - lower_bound_frentes_arca)) ~ 50,
      TRUE ~ 0
    ),
    # Calificación para duration
    score_duration = case_when(
      duration >= lower_bound_duration & duration <= upper_bound_duration ~ 100,
      duration >= (lower_bound_duration - 0.05 * (upper_bound_duration - lower_bound_duration)) &
        duration <= (upper_bound_duration + 0.05 * (upper_bound_duration - lower_bound_duration)) ~ 50,
      TRUE ~ 0
    ),
    # Calificación para enfriador_total
    score_enfriador_total = case_when(
      enfriador_total >= lower_bound_enfriadores & enfriador_total <= upper_bound_enfriadores ~ 100,
      enfriador_total >= (lower_bound_enfriadores - 0.03 * (upper_bound_enfriadores - lower_bound_enfriadores)) &
        enfriador_total <= (upper_bound_enfriadores + 0.03 * (upper_bound_enfriadores - lower_bound_enfriadores)) ~ 50,
      TRUE ~ 0
    ),
    
    # Calificación para Num.Scenes
    score_NumScenes = case_when(
      Num.Scenes >= lower_bound_NumScenes & Num.Scenes <= upper_bound_NumScenes ~ 100,
      Num.Scenes >= (lower_bound_NumScenes - 0.1 * (upper_bound_NumScenes - lower_bound_NumScenes)) &
        Num.Scenes <= (upper_bound_NumScenes + 0.1 * (upper_bound_NumScenes - lower_bound_NumScenes)) ~ 50,
      TRUE ~ 0
    ),
    # Calificación para Scenes_Amb
    score_ScenesAmb = case_when(
      Scenes_Amb >= lower_bound_ScenesAmb & Scenes_Amb <= upper_bound_ScenesAmb ~ 100,
      Scenes_Amb >= (lower_bound_ScenesAmb - 0.1 * (upper_bound_ScenesAmb - lower_bound_ScenesAmb)) &
        Scenes_Amb <= (upper_bound_ScenesAmb + 0.1 * (upper_bound_ScenesAmb - lower_bound_ScenesAmb)) ~ 50,
      TRUE ~ 0
    ),
    # Calificación para Scenes_Frio
    score_ScenesFrio = case_when(
      Scenes_Frio >= lower_bound_ScenesFrio & Scenes_Frio <= upper_bound_ScenesFrio ~ 100,
      Scenes_Frio >= (lower_bound_ScenesFrio - 0.1 * (upper_bound_ScenesFrio - lower_bound_ScenesFrio)) &
        Scenes_Frio <= (upper_bound_ScenesFrio + 0.1 * (upper_bound_ScenesFrio - lower_bound_ScenesFrio)) ~ 50,
      TRUE ~ 0
    )
  )

# Verificar el resultado
head(master_evaluado)
```

# Imputación de Territorio según coordenadas
```{r}
# Asegurándonos que SessionUID es del mismo tipo en ambos dataframes
df_session$SessionUID <- as.character(df_session$SessionUId)
master_evaluado$SessionUID <- as.character(master_evaluado$SessionUID)

# Uniendo las coordenadas de df_session a master_calidad
master_evaluado <- master_evaluado %>%
  left_join(df_session %>% select(SessionUID, latitude, longitude),
            by = "SessionUID")
```

```{r}
# Asegurarse de que las variables están en el formato correcto
master_evaluado$frentes_total <- as.numeric(master_evaluado$frentes_total)

# Ordenar los niveles de 'tamaño_homologado'
master_evaluado$tamaño_homologado <- factor(master_evaluado$tamaño_homologado, 
                                            levels = c("Chico", "Mediano", "Grande", NA))

# Crear un gráfico de caja para mostrar la dispersión de 'frentes_total' agrupado por 'tamaño_homologado'
ggplot(master_evaluado, aes(x = tamaño_homologado, y = frentes_total)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Dispersión de la variable 'frentes_total' por 'tamaño_homologado'",
       x = "Tamaño Homologado",
       y = "Frentes Total")
```


----------- GUARDAR DATA PROCESADA

```{r message=FALSE, warning=FALSE}
# Obtener el mes y año actual automáticamente
mes_actual <- format(Sys.Date(), "%B_%Y")

# Leer o inicializar el archivo del mes
archivo_csv_mes <- paste0("data_procesada/", mes_actual, ".csv")

# Cargar datos del mes si el archivo existe, o crear un dataframe vacío si no existe
if (file.exists(archivo_csv_mes)) {
  datos_mes <- read_csv(archivo_csv_mes)
} else {
  datos_mes <- tibble() # Crear un tibble vacío
}

# Convertir todas las columnas comunes a character
columns_to_convert <- intersect(names(master_evaluado), names(datos_mes))
master_evaluado[columns_to_convert] <- lapply(master_evaluado[columns_to_convert], as.character)
datos_mes[columns_to_convert] <- lapply(datos_mes[columns_to_convert], as.character)

# Concatenar los datos nuevos con los antiguos
datos_combinados <- bind_rows(master_evaluado, datos_mes)

# Calcular la cantidad de NAs para cada fila antes de agrupar
datos_combinados <- datos_combinados %>%
  mutate(cantidad_na = rowSums(is.na(.)))

# Seleccionar la entrada con menos NA para cada SessionUID
datos_actualizados <- datos_combinados %>%
  group_by(SessionUID) %>%
  arrange(cantidad_na) %>%
  slice(1) %>%
  ungroup() %>%
  select(-cantidad_na)

# Guardar los datos actualizados en el archivo CSV del mes
write_csv(datos_actualizados, archivo_csv_mes)

# Mensaje de confirmación
cat("Los datos para", mes_actual, "han sido actualizados y guardados en", archivo_csv_mes, "\n")
```


-----------EXTRACTOS-----------
# Sabana de Calidad Madre
```{r message=FALSE, warning=FALSE}
# Ruta a la carpeta con los archivos procesados
ruta_carpeta <- "data_procesada/"

# Obtener lista de todos los archivos CSV en la carpeta
archivos <- dir_ls(ruta_carpeta, regexp = "\\.csv$")

# Función para leer un archivo CSV y asegurarse de que todas las columnas sean del mismo tipo
leer_y_convertir <- function(archivo) {
  df <- read_csv(archivo)
  # Convertir todas las columnas a character para evitar conflictos
  df[] <- lapply(df, as.character)
  return(df)
}

# Leer cada archivo, convertir columnas y combinarlos en un solo dataframe
master_calidad <- map_dfr(archivos, leer_y_convertir)

# Asumiendo que master_calidad es tu dataframe
master_calidad <- master_calidad %>%
  mutate(
    duration = as.numeric(duration),
    distance = as.numeric(distance),
    frentes_total = as.numeric(frentes_total),
    frentes_arca = as.numeric(frentes_arca),
    sovi = as.numeric(sovi),
    flag_trigger = as.numeric(flag_trigger),
    enfriador_total = as.numeric(enfriador_total),
    score_frentes_total = as.numeric(score_frentes_total),
    score_frentes_arca = as.numeric(score_frentes_arca),
    score_duration = as.numeric(score_duration),
    score_enfriador_total = as.numeric(score_enfriador_total),
    latitude = as.numeric(latitude),
    longitude = as.numeric(longitude),
    estatus = as.numeric(estatus),
    lower_bound_frentes = as.numeric(lower_bound_frentes),
    upper_bound_frentes = as.numeric(upper_bound_frentes),
    lower_bound_enfriadores = as.numeric(lower_bound_enfriadores),
    upper_bound_enfriadores = as.numeric(upper_bound_enfriadores),
    lower_bound_duration = as.numeric(lower_bound_duration),
    upper_bound_duration = as.numeric(upper_bound_duration),
    score_NumScenes = as.numeric(score_NumScenes),
    score_ScenesAmb = as.numeric(score_ScenesAmb),
    score_ScenesFrio = as.numeric(score_ScenesFrio)
  )

# Guardar el dataframe combinado como un archivo Excel
write.xlsx(master_calidad, "master_calidad.xlsx")

# Mensaje de confirmación
cat("El archivo 'master_calidad.xlsx' ha sido guardado con éxito.\n")
```

#Extracto Clientes
```{r}
extracto_clientes <- master_clientes %>%
  select(
    tradechannelcode, sub_canal_isscom, salesgroupcode, salesorganizationcode,
    country, subclientcode, tamaño, ruta_preventa_oficial, customercode, customername, 
    modelo_de_servicio_ruta, isactive, zona...13, `territorio...14`, salesterritorycode
  )

# Guardar el dataframe en un archivo Excel
write.xlsx(extracto_clientes, "extracto_clientes.xlsx")

# Si quieres confirmar que el archivo se ha guardado
cat("Archivo 'extracto_clientes.xlsx' guardado con éxito.")
```

------- BUSQUEDA ----------

### Actual
```{r}
result_dplyr <- df_actual %>%
  filter(SessionUID == "d5d287fa-89cf-4e4c-88ef-97dfbb46bc1a")

# Mostrando el resultado
print(result_dplyr)
```

```{r}
summary(master_evaluado)
```


---------- DIVISION DE DATA POR MES -------------
```{r message=FALSE, warning=FALSE}
# Ruta al archivo Excel
archivo_excel <- "ICD - Indice de Calidad de Data.xlsx"

# Leer los datos de la pestaña "Data"
df <- read_excel(archivo_excel, sheet = "Data")

# Convertir la columna "Survey End Time" a tipo fecha
df$Survey_End_Time <- as.Date(df$`Survey End Time`, format = "%m/%d/%Y")

# Crear una columna con el mes y año
df$Mes_Año <- format(df$Survey_End_Time, "%B_%Y")

# Crear una carpeta para almacenar los datos procesados si no existe
ruta_carpeta <- "data_procesada"
if (!dir.exists(ruta_carpeta)) {
  dir.create(ruta_carpeta, recursive = TRUE)
  
  # Verificar si la carpeta fue creada
  if (!dir.exists(ruta_carpeta)) {
    stop("La carpeta no pudo ser creada. Verifica los permisos.")
  }
}

# Función para guardar los archivos CSV por mes
guardar_csv_por_mes <- function(data, mes, ruta_carpeta) {
  ruta_archivo <- file.path(ruta_carpeta, paste0(mes, ".csv"))
  write.csv(data, ruta_archivo, row.names = FALSE)
  cat("Archivo guardado:", ruta_archivo, "\n")
}

# Aplicar la función a cada mes
lapply(unique(df$Mes_Año), function(mes) {
  datos_mes <- filter(df, Mes_Año == mes)
  if(nrow(datos_mes) > 0) {
    guardar_csv_por_mes(datos_mes, mes, ruta_carpeta)
  } else {
    cat("No hay datos para el mes:", mes, "\n")
  }
})

cat("Todos los archivos han sido creados.\n")
```

-------- QUERIES PERSONALIZADOS --------
```{r}
# Asegúrate de que 'Survey End Time' es de tipo Date
master_evaluado$`Survey End Time` <- as.Date(master_evaluado$`Survey End Time`, format = "%Y-%m-%d")

# Crear una nueva columna con la fecha del lunes de la semana correspondiente
master_evaluado <- master_evaluado %>%
  mutate(
    start_of_week = floor_date(`Survey End Time`, unit = "week"),
    week_label = format(start_of_week, "%d %b"),
    week_order = as.numeric(start_of_week)
  )

# Agrupar por 'salesterritorycode' y 'week_label', y contar 'SessionUID' únicos
conteo_semanal <- master_evaluado %>%
  group_by(salesterritorycode, week_label) %>%
  summarise(total_count = n_distinct(SessionUID), .groups = 'drop') %>%
  ungroup()

# Ordenar el dataframe por 'salesterritorycode' y 'week_order'
# Nota: Es importante usar 'week_order' solo para ordenar antes del pivot_wider y no incluirlo en el pivot_wider para evitar duplicados.
conteo_semanal <- conteo_semanal %>%
  arrange(salesterritorycode, week_order)

# Pivotar los datos para tener las etiquetas de semanas como columnas y 'salesterritorycode' como filas
tabla_conteo_semanal <- conteo_semanal %>%
  pivot_wider(
    names_from = week_label,
    values_from = total_count,
    values_fill = list(total_count = 0)
  )

# Ordenar las columnas por fecha usando 'week_order' de 'master_evaluado'
column_order <- unique(master_evaluado$week_label[order(master_evaluado$week_order)])
tabla_conteo_semanal <- tabla_conteo_semanal %>%
  select(salesterritorycode, all_of(column_order))

# Guardar el dataframe en un archivo Excel
write.xlsx(tabla_conteo_semanal, "/mnt/data/tabla_conteo_semanal.xlsx")
```

x

