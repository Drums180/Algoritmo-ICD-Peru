---
title: "Indice de Calidad - Perú"
author: "David Dominguez - A01570975"
date: "2023-09-14"
output: html_document
---

![Data Quality Algorithm](src/data_quality.png)

# Algoritmo de Validación de Calidad
El **Algoritmo de validación de Calidad** es un modelo diseñado para unificar y automatizar los procesos de limpieza, transformación, unificación y validación de datos en la mayor medida posible. Su objetivo principal es asegurar que la información sea confiable y representativa de la realidad, facilitando la toma de decisiones basadas en datos precisos y fidedignos.

Este algoritmo forma parte del proyecto **CENSO MONITOREO 2023**, el cual propuso el uso de tecnología de reconocimiento fotográfico para habilitar a la fuerza de ventas como auditores. De esta manera, se busca recopilar información de manera eficiente, rápida y orientada a datos para tomar decisiones informadas. El programa, iniciado en septiembre de 2023, se centró inicialmente en el canal tradicional, incluyendo subcanales como abarrotes y fruterías, que son esenciales para los negocios que consumen nuestros productos a nivel nacional.

## Llamar Librerias
Esta sección está dedicada a cargar todas las librerías necesarias para ejecutar el **Algoritmo de Validación de Calidad**. Las librerías seleccionadas facilitan una amplia gama de operaciones cruciales para el procesamiento y análisis de datos:

- `dplyr`: Esencial para la manipulación y transformación eficiente de datos.
- `readr`: Permite la lectura rápida y sencilla de datos tabulares.
- `readxl`: Utilizado para importar datos desde archivos Excel.
- `tidyverse`: Un compendio de paquetes para ciencia de datos que incluye `dplyr`, `ggplot2`, entre otros, proporcionando herramientas completas para el análisis de datos.
- `fs`: Facilita el manejo del sistema de archivos.
- `purrr`: Permite la programación funcional para iterar operaciones en listas y vectores.
- `openxlsx`: Ofrece capacidades avanzadas para la lectura y escritura de archivos Excel.
- `sf`: Especializado en el manejo de datos espaciales y geográficos.
- `httr`: Esencial para la comunicación con APIs web, permitiendo interactuar con otros servicios de software.
- `jsonlite`: Maneja y procesa datos en formato JSON.
- `ggplot2`: Proporciona herramientas avanzadas para la visualización gráfica de datos.
- `stringr`: Simplifica la manipulación y gestión de texto.

Estas herramientas son fundamentales para garantizar la ejecución eficiente y efectiva del algoritmo, permitiendo una gestión de datos robusta y fiable que respalde la toma de decisiones informadas.

```{r message=FALSE, warning=FALSE}
library(dplyr)    # Manipulación de datos
library(readr)    # Lectura de datos tabulares
library(readxl)   # Lectura de archivos Excel
library(tidyverse) # Ciencia de datos (incluye dplyr, ggplot2, etc.)
library(fs)       # Manejo del sistema de archivos
library(purrr)    # Programación funcional
library(openxlsx) # Lectura/escritura de Excel avanzada
library(sf)       # Datos espaciales
library(httr)     # Comunicación con APIs web
library(jsonlite) # Trabajo con JSON
library(ggplot2)  # Visualización de datos
library(stringr)
```

## Carga de Base de Datos
La carga de bases de datos es esencial para la operación del **Algoritmo de Validación de Calidad**. Este proceso implica la recolección y combinación de diversos conjuntos de datos, cada uno crucial para distintas etapas del análisis. A continuación, se detalla la funcionalidad y el propósito de cada base de datos utilizada:


### Quality Data
Esta sección del código se encarga de combinar múltiples archivos de datos, ya sean CSV o Excel, provenientes de diferentes fuentes. El propósito es unificar esta información en un solo dataframe, aplicando transformaciones específicas como la conversión de tipos de datos para garantizar la consistencia y la integridad de la información. Las bases de datos procesadas aquí incluyen:

- **actual**: Contiene información procesada de reconocimiento fotográfico con los facings de productos de Arca Continental y otros.
- **session**: Registra datos de cada sesión de toma de fotos, incluyendo el usuario, el cliente, y las coordenadas geográficas.
- **survey**: Filtra los datos de la sesión por tipo de encuesta, centrándose en censo y otros tipos específicos.
- **scenes**: Detalla el tipo de foto tomada, como imágenes de refrigeradores o estantes, clasificando si son de ambiente o enfriadores.
```{r message=FALSE, warning=FALSE}
setwd("fuentes_datos/")  # Establecer ruta de trabajo

# Función para leer y combinar archivos de una carpeta
combine_files <- function(path, file_type = c("csv", "excel")) {
  
  # Obtener lista de archivos en el directorio especificado
  files <- list.files(path, full.names = TRUE)
  
  df_combined <- data.frame()
  
  process_file <- function(file) {
  tryCatch({
    if (grepl("\\.csv$", file)) {
      data <- read_csv(file)
    } else if (grepl("\\.(xlsx|xls|XLSX|XLS)$", file)) {
      data <- read_excel(file)
    } else {
      stop("Tipo de archivo no soportado")
    }
    
    # Lista de columnas que deben ser double
    cols_to_double <- c("Duration(Sec)", "longitude", "latitude", "SessionEndLongitude", "SessionEndLatitude")
    
    # Convertir todas las columnas a character, excepto las especificadas
    data[] <- lapply(names(data), function(col) {
      if (col %in% cols_to_double) {
        as.numeric(as.character(data[[col]]))
      } else {
        as.character(data[[col]])
      }
    })
    
    return(data)
  }, error = function(e) {
    print(paste("Error al procesar el archivo:", file))  # Imprimir el archivo con error
    print(e)  # Imprimir el error específico
    return(NULL)
  })
}

  if ("csv" %in% file_type) {
    files_csv <- grep("\\.csv$", files, value = TRUE)
    if(length(files_csv) > 0){
      df_list_csv <- lapply(files_csv, process_file)
      df_combined <- bind_rows(df_combined, do.call(bind_rows, df_list_csv))
    }
  }
  
  if ("excel" %in% file_type) {
    files_xls <- grep("\\.(xlsx|xls|XLSX|XLS)$", files, value = TRUE)
    if(length(files_xls) > 0){
      df_list_xls <- lapply(files_xls, process_file)
      df_combined <- bind_rows(df_combined, do.call(bind_rows, df_list_xls))
    }
  }
  
  # Eliminar filas duplicadas
  if(nrow(df_combined) > 0) {
    df_combined <- df_combined %>% distinct()
  }
  
  return(df_combined)
}

# Crear data frames para cada carpeta
df_actual <- combine_files("actual/")
df_session <- combine_files("session/")
df_survey <- combine_files("survey/")
df_scenes <- combine_files("scenes/")
```

### Master Clientes
El código aquí presente está destinado a crear un 'master' de clientes a partir de varios archivos, los cuales se limpian y estandarizan para asegurar una nomenclatura consistente. Este 'master' es fundamental para realizar cruces de datos y análisis demográficos o de mercado sobre los clientes. Combina y deduplica registros para formar una base robusta y actualizada de información cliente.

```{r message=FALSE, warning=FALSE}
# Leer todos los archivos en fuentes_datos/clientes, excepto el de clientes que leeremos directamente
all_dfs <- read_excel("fuentes_datos/clientes/MD TRAD PERU 11.23.xlsx")

# Función para limpiar y estandarizar nombres de columnas
limpiar_nombres <- function(nombres) {
  nombres <- tolower(nombres)
  nombres <- chartr("áéíóú", "aeiou", nombres)
  nombres <- gsub(" ", "_", nombres)
  return(nombres)
}

# Extraer nombres de columnas, limpiarlos y reasignarlos
nombres_columnas <- names(all_dfs)
nombres_columnas_limpios <- limpiar_nombres(nombres_columnas)
names(all_dfs) <- nombres_columnas_limpios

# Eliminar duplicados, manteniendo el último
all_dfs <- all_dfs %>%
  arrange(desc(codigo_cliente)) %>% 
  distinct(codigo_cliente, .keep_all = TRUE)

# Hacer el left_join con clientes
master_clientes <- all_dfs 

# Verificar el resultado
head(master_clientes)
```

### Parametros
Finalmente, esta sección carga parámetros específicos que son necesarios para el funcionamiento del algoritmo. Estos parámetros pueden incluir configuraciones, límites y otros valores clave que influyen en el procesamiento y análisis de los datos.

```{r message=FALSE, warning=FALSE}
# Cargar la hoja "data" del archivo "parametros"
parametros_data <- read_excel("parametros.xlsx", sheet = "data")

# Verificar la carga de datos
head(parametros_data)
```

Cada una de estas bases de datos juega un rol crucial en asegurar que el **Algoritmo de Validación de Calidad** funcione de manera óptima, proporcionando resultados precisos y confiables para la toma de decisiones basada en datos.

## Sabana de Analisis

Esta sección aborda el análisis detallado de los datos recolectados, enfocándose en tres aspectos principales: tiempo, coordenadas y frentes. Cada subsección aplica transformaciones y cálculos específicos para extraer métricas clave que apoyan la interpretación y las decisiones basadas en los datos.

### Tiempo
En esta subsección, convertimos la duración de las encuestas de segundos a minutos para facilitar la interpretación. Además, creamos una variable binaria que indica si el estatus de la encuesta es 'Completa'. Estas transformaciones permiten analizar más eficientemente el tiempo dedicado a cada sesión y su finalización.
```{r}
df_survey <- df_survey %>%
  mutate(duration = `Duration(Sec)` / 60) %>%
  mutate(estatus = if_else(Status == "Complete", 1, 0))

# Verificar los cambios
head(df_survey)
```

### Coordenadas 
Calculamos la distancia entre dos puntos geográficos utilizando la fórmula del haversine, que es esencial para entender la dispersión geográfica de las sesiones y clientes. Posteriormente, integramos datos de clientes para calcular estas distancias y identificar valores atípicos, los cuales se reemplazan por 0 para normalizar el análisis.

```{r}
# Función del haversine para calcular distancia entre dos puntos de latitud y longitud
haversine <- function(lon1, lat1, lon2, lat2) {
  R <- 6371000  # Radio de la Tierra en metros
  phi1 <- lat1 * (pi / 180)
  phi2 <- lat2 * (pi / 180)
  delta_phi <- (lat2 - lat1) * (pi / 180)
  delta_lambda <- (lon2 - lon1) * (pi / 180)
  
  a <- sin(delta_phi / 2)^2 + cos(phi1) * cos(phi2) * sin(delta_lambda / 2)^2
  c <- 2 * atan2(sqrt(a), sqrt(1 - a))
  
  d <- R * c  # Distancia en metros
  return(d)
}

# Convertir las columnas relevantes a character
df_session$OutletCode <- as.character(df_session$OutletCode)
master_clientes$codigo_cliente <- as.character(master_clientes$codigo_cliente)

# Realizar el join y calcular la distancia usando la función haversine
df_session <- df_session %>%
  left_join(select(master_clientes, codigo_cliente, latitude, longitude),
            by = c("OutletCode" = "codigo_cliente")) %>%
  mutate(
    distance = mapply(haversine, longitude, latitude, SessionEndLongitude, SessionEndLatitude)
  )

# Calculamos los cuartiles y el rango intercuartil de la distancia
Q1 <- quantile(df_session$distance, 0.25, na.rm = TRUE)
Q3 <- quantile(df_session$distance, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1

# Reemplazar valores atípicos por 0
df_session$distance[df_session$distance < (Q1 - 1.5 * IQR) | 
                    df_session$distance > (Q3 + 1.5 * IQR)] <- 0

# Identificar valores atípicos
outliers <- df_session$distance[df_session$distance < (Q1 - 1.5 * IQR) | 
                                df_session$distance > (Q3 + 1.5 * IQR)]

# Imprimir valores atípicos
print(outliers)


# Verificar los cambios
head(df_session)
```

### Frentes
Esta subsección evalúa la presencia y el tipo de productos en puntos de venta, calculando el número de frentes de productos propios y competidores. También determinamos el SOVI (Share of Visual Inventory), que mide la visibilidad de la marca en relación con los competidores.

```{r}
frentes_df <- df_actual %>%
  group_by(SessionUID) %>%
  summarise(
    # Cálculo de num_frentes
    num_frentes = sum(ProductName != "Foreign" & IsEmpty == FALSE),
    # Cálculo de frentes_com
    frentes_com = sum(ProductName != "Foreign" & IsForeign == TRUE),
    # Cálculo de frentes_total
    frentes_total = sum(ProductName != "Foreign" & IsEmpty == FALSE & BeverageType != "Lacteos")
  ) %>%
  mutate(
    # Cálculo de frentes_arca
    frentes_arca = frentes_total - frentes_com,
    # Cálculo de sovi
    sovi = ifelse(frentes_total == 0, 0, frentes_arca / frentes_total)
  )

# Verificar el nuevo dataframe
head(frentes_df)
```

### Enfriadores
Esta subsección analiza y cuenta los tipos de enfriadores presentes en las escenas capturadas. Utilizamos el agrupamiento por `SessionUID` y `SubSceneType` para contar cada tipo de enfriador y luego transformar estos conteos en columnas específicas. Este análisis es vital para entender la distribución y prevalencia de nuestros enfriadores en comparación con los cedidos por la competencia en los puntos de venta.

```{r}
result_enfriadores <- df_scenes %>%
  # Agrupar por SessionUID y SubSceneType y contar
  group_by(SessionUID, SubSceneType) %>%
  count() %>%
  # Transforma valores de SubSceneType en columnas individuales
  spread(key = SubSceneType, value = n, fill = 0) %>% # fill = 0 para llenar con 0s donde no haya conteos
  ungroup()

# Funcion para sumar Enfriadores 

# Genera las columnas si no existen
if (!"EDF ACL" %in% names(result_enfriadores)) {
  result_enfriadores$`EDF ACL` <- 0
}

# Suma las columnas
result_enfriadores <- result_enfriadores %>%
  mutate(enfriador_total = `EDF ACL`)

# Verificar el resultado
head(result_enfriadores)
```

### Scenes
En esta subsección, combinamos datos de `df_actual` con `df_scenes` para enriquecer el análisis de las escenas capturadas con el tipo de subescena. Calculamos el número de frentes por cada tipo de subescena y aplicamos filtros para identificar escenas con significancia estadística, excluyendo ciertos tipos bajo criterios específicos.

```{r}
# Calculando el número total de escenas por SessionUID
df_total_scenes <- df_scenes %>%
  group_by(SessionUID) %>%
  summarise(Num.Scenes = n(), .groups = "drop")

# Calculando el número de escenas de tipo 'Ambiente'
df_ambiente <- df_scenes %>%
  filter(SceneType == "Ambiente") %>%
  group_by(SessionUID) %>%
  summarise(Scenes_Amb = n(), .groups = "drop")

# Calculando el número de escenas de tipo 'Frio'
df_frio <- df_scenes %>%
  filter(SceneType == "Frio") %>%
  group_by(SessionUID) %>%
  summarise(Scenes_Frio = n(), .groups = "drop")

# Uniendo los dataframes
df_scenenum <- df_total_scenes %>%
  left_join(df_ambiente, by = "SessionUID") %>%
  left_join(df_frio, by = "SessionUID")

# Reemplazando NA con 0 para las columnas Scenes_Amb y Scenes_Frio
df_scenenum$Scenes_Amb[is.na(df_scenenum$Scenes_Amb)] <- 0
df_scenenum$Scenes_Frio[is.na(df_scenenum$Scenes_Frio)] <- 0

# Mostrando el resultado final
print(df_scenenum)
```

### Flags
Esta subsección se dedica a la detección y manejo de 'flags' o indicadores en la calidad de imagen. Procesamos la columna `ImageQuality` para identificar y resumir los flags detectados, aplicando una lógica específica para transformar y analizar estos datos, lo cual es crucial para garantizar la integridad y utilidad de las imágenes en análisis posteriores.

```{r}
# Creación del vector de valores de flag
flag_values <- c(2, 4, 13, 14, 15, 16, 26, 27, 32, 36, 39, 31)
#-------------------------------------------------------------

# Función para dividir la cadena en pares y añadir comas
format_image_quality <- function(value) {
  # Divide la cadena en pares de caracteres
  split_values <- str_extract_all(value, ".{1,2}")[[1]]
  # Une los valores con comas
  paste(split_values, collapse = ",")
}

# Aplica la función a la columna ImageQuality
df_scenes$ImageQuality <- sapply(df_scenes$ImageQuality, format_image_quality)

# Ahora procede con la agrupación y concatenación
result_scenes <- df_scenes %>%
  group_by(SessionUID) %>%
  summarise(ImageQuality = paste(unique(ImageQuality), collapse = ",")) %>%
  ungroup()

#--------------------------------------------------------------
# Función para detectar las flags en la columna ImageQuality
detected_flags <- function(quality_string) {
  quality_values <- as.numeric(unlist(strsplit(quality_string, ",")))  # Divide la cadena y convierte a numéricos
  detected <- quality_values[quality_values %in% flag_values]
  if(length(detected) > 0) {
    return(paste(detected, collapse = ","))
  } else {
    return(NA_character_)
  }
}

# Función modificada para contar las flags y asignar -10 por cada flag presente
check_flags <- function(quality_string) {
  quality_values <- as.numeric(unlist(strsplit(quality_string, ",")))  # Divide la cadena y convierte a numéricos
  flag_count <- sum(quality_values %in% flag_values)  # Cuenta cuántas flags están presentes
  return(100 + (-10 * flag_count))  # Asigna -10 por cada flag
}

# Función para detectar las flags en la columna ImageQuality (ya existente en tu código)
detect_flags <- function(quality_string) {
  quality_values <- as.numeric(unlist(strsplit(quality_string, ",")))  # Divide la cadena y convierte a numéricos
  detected <- quality_values[quality_values %in% flag_values]
  if(length(detected) > 0) {
    return(paste(detected, collapse = ","))
  } else {
    return(NA_character_)
  }
}

# Aplicar ambas funciones a la columna ImageQuality de result_scenes
result_scenes$flag_trigger <- sapply(result_scenes$ImageQuality, check_flags)
result_scenes$detected_flags <- sapply(result_scenes$ImageQuality, detect_flags)

# Verificar el resultado
head(result_scenes)
```

## Empalme de Información
Esta sección se centra en la gestión y combinación de datos históricos con datos recientes, lo cual es crucial para realizar análisis longitudinales y mantener la consistencia y continuidad de la información a lo largo del tiempo. 

### Cargar Data Historica
Inicialmente, verificamos y cargamos todos los archivos históricos disponibles en un formato estructurado. Esto incluye asegurar que las identificaciones de los puntos de venta sean tratadas como caracteres para mantener la integridad de los datos. Los datos históricos se seleccionan y preparan con una serie de columnas específicas para futuros análisis y comparaciones.

```{r message=FALSE, warning=FALSE}
# Ruta a la carpeta con archivos históricos procesados
ruta_carpeta <- "data_procesada/"

# Verificar si hay archivos históricos
archivos_historicos <- list.files(path = ruta_carpeta, pattern = "\\.csv$", full.names = TRUE)

if (length(archivos_historicos) > 0) {
  # Cargar y combinar todos los archivos históricos en un dataframe
  datos_historicos <- map_dfr(archivos_historicos, function(file) {
    datos <- read_csv(file)
    datos$`Outlet Code` <- as.character(datos$`Outlet Code`)  # Asegurar que Outlet Code sea siempre un carácter
    select(datos,
      SessionUID, SurveyType, User, `Outlet Code`, 
      region_comercial, tamaño_homologado, canal_isscom, subcanal_isscom,
      duration, distance, 
      frentes_total, frentes_arca, sovi, flag_trigger, enfriador_total, 
      Num.Scenes, Scenes_Amb, Scenes_Frio, detected_flags,
      subregion_comercial, territorio, `Survey End Time`, estatus
    )
  })
} else {
  # Inicializar un dataframe vacío con las columnas esperadas
  datos_historicos <- tibble(
    SessionUID = character(), SurveyType = character(), User = character(), `Outlet Code` = character(),
    region_comercial = character(), tamaño_homologado = character(), canal_isscom = character(), subcanal_isscom = character(),
    duration = numeric(), distance = numeric(), 
    frentes_total = numeric(), frentes_arca = numeric(), sovi = numeric(), flag_trigger = numeric(), enfriador_total = numeric(),
    Num.Scenes = numeric(), Scenes_Amb = numeric(), Scenes_Frio = numeric(), detected_flags = character(),
    subregion_comercial = character(), territorio = character(), `Survey End Time` = Date(), estatus = numeric()
  )
}
```

### Data Nueva e Historica Combinada
Después de preparar los datos históricos, procedemos a integrarlos con los datos nuevos recolectados en encuestas recientes. Esto asegura que tengamos un conjunto de datos completo y actualizado, con `Survey End Time` estandarizado a tipo fecha, permitiendo análisis temporales precisos y efectivos.

```{r}
# Preparar datos nuevos de df_survey con las columnas relevantes
nuevos_datos_survey <- df_survey %>%
  mutate(
    `Survey End Time` = as.Date(substr(`Survey End Time`, 1, 10), format = "%d/%m/%Y"),
    SessionUID = `Session Uid`, 
    SurveyType = `Survey Type`, 
    duration = `Duration(Sec)`
  ) %>%
  select(
    SessionUID, SurveyType, User, `Outlet Code`, duration, `Survey End Time`, estatus
  )

# Combinar datos históricos con nuevos datos de survey, ahora ambos con 'Survey End Time' como tipo Date
datos_combinados <- bind_rows(datos_historicos, nuevos_datos_survey) %>%
  distinct(SessionUID, .keep_all = TRUE)
```

## Sabana Calidad Semilla 
Esta sección se encarga de imputar datos faltantes en el conjunto de datos combinado utilizando información de varias fuentes para garantizar la integridad y la completitud de los datos. Las imputaciones se realizan en campos clave que son críticos para el análisis posterior y la toma de decisiones.

```{r}
# Imputaciones desde df_session para 'distance'
datos_combinados <- datos_combinados %>%
  mutate(
    distance = ifelse(is.na(distance),
                      df_session$distance[match(SessionUID, df_session$SessionUId)],
                      distance)
  )

# Imputaciones desde frentes_df para 'frentes_total', 'frentes_arca', y 'sovi'
datos_combinados <- datos_combinados %>%
  mutate(
    frentes_total = ifelse(is.na(frentes_total),
                           frentes_df$frentes_total[match(SessionUID, frentes_df$SessionUID)],
                           frentes_total),
    frentes_arca = ifelse(is.na(frentes_arca),
                          frentes_df$frentes_arca[match(SessionUID, frentes_df$SessionUID)],
                          frentes_arca),
    sovi = ifelse(is.na(sovi),
                  frentes_df$sovi[match(SessionUID, frentes_df$SessionUID)],
                  sovi)
  )

# Imputaciones desde result_scenes para 'flag_trigger' y 'detected_flags'
datos_combinados <- datos_combinados %>%
  mutate(
    flag_trigger = ifelse(is.na(flag_trigger),
                          result_scenes$flag_trigger[match(SessionUID, result_scenes$SessionUID)],
                          flag_trigger),
    detected_flags = ifelse(is.na(detected_flags),
                            result_scenes$detected_flags[match(SessionUID, result_scenes$SessionUID)],
                            detected_flags)
  )

# Imputaciones desde result_enfriadores para 'enfriador_total'
datos_combinados <- datos_combinados %>%
  mutate(
    enfriador_total = ifelse(is.na(enfriador_total),
                             result_enfriadores$enfriador_total[match(SessionUID, result_enfriadores$SessionUID)],
                             enfriador_total)
  )

# Imputaciones para datos de clientes
datos_combinados <- datos_combinados %>%
  mutate(
    tamaño_homologado = ifelse(is.na(tamaño_homologado),
                    master_clientes$tamaño_homologado[match(`Outlet Code`, master_clientes$codigo_cliente)],
                    tamaño_homologado),
    region_comercial = ifelse(is.na(region_comercial),
                                   master_clientes$region_comercial[match(`Outlet Code`, master_clientes$codigo_cliente)],
                                   region_comercial),
    canal_isscom = ifelse(is.na(canal_isscom),
                              master_clientes$canal_isscom[match(`Outlet Code`, master_clientes$codigo_cliente)],
                              canal_isscom),
    subcanal_isscom = ifelse(is.na(subcanal_isscom),
                              master_clientes$subcanal_isscom[match(`Outlet Code`, master_clientes$codigo_cliente)],
                              subcanal_isscom),
    subregion_comercial = ifelse(is.na(subregion_comercial),
                                master_clientes$subregion_comercial[match(`Outlet Code`, master_clientes$codigo_cliente)],
                                subregion_comercial),
    territorio = ifelse(is.na(territorio),
                        master_clientes$territorio[match(`Outlet Code`, master_clientes$codigo_cliente)],
                        territorio)
  )

# Imputaciones desde df_summary para 'Num.Scenes', 'Scenes_Amb', y 'Frio_Scenes'
datos_combinados <- datos_combinados %>%
  mutate(
    Num.Scenes = ifelse(is.na(Num.Scenes),
                        df_scenenum$Num.Scenes[match(SessionUID, df_scenenum$SessionUID)],
                        Num.Scenes),
    Scenes_Amb = ifelse(is.na(Scenes_Amb),
                        df_scenenum$Scenes_Amb[match(SessionUID, df_scenenum$SessionUID)],
                        Scenes_Amb),
    Scenes_Frio = ifelse(is.na(Scenes_Frio),
                        df_scenenum$Scenes_Frio[match(SessionUID,df_scenenum$SessionUID)],
                        Scenes_Frio)
)

master_calidad <- datos_combinados
```

### Eliminar Directorio
Después de consolidar los datos necesarios, limpiamos los directorios para mantener un entorno de trabajo organizado y eficiente. Esta práctica asegura que solo se retengan los archivos esenciales, facilitando la gestión de datos y la prevención de errores de procesamiento debido a la presencia de archivos obsoletos o innecesarios.

```{r}
# Función para limpiar directorio de archivos individuales, manteniendo solo el combinado
clean_directory <- function(path) {
  combined_file_path <- file.path(path, "combined.csv")
  
  # Listar todos los archivos excepto el archivo combinado
  files_to_delete <- setdiff(list.files(path, full.names = TRUE), combined_file_path)
  
  # Eliminar archivos
  file.remove(files_to_delete)
}

# Limpiar directorios de archivos individuales
clean_directory("fuentes_datos/session/")
clean_directory("fuentes_datos/survey/")
clean_directory("fuentes_datos/actual/")
clean_directory("fuentes_datos/scenes/")
```

## Correción de Datos

### Flags Inducidas 
En esta subsección, implementamos correcciones específicas para las flags en los datos, basándonos en criterios predeterminados que reflejan inconsistencias o condiciones anómalas en los datos. Estas correcciones son críticas para mantener la integridad de los análisis y asegurar que las decisiones basadas en estos datos sean sólidas y fiables.

```{r}
# 1. Contar los SceneUID distintos para cada SessionUID
scene_count_df <- df_actual %>%
  group_by(SessionUID) %>%
  summarise(min_frentes = n_distinct(SceneUID) * 3) 

# Unir temporalmente con master_calidad para aplicar las condiciones
master_calidad_temp <- left_join(master_calidad, scene_count_df, by = "SessionUID")

# Actualizar las flags de acuerdo con las condiciones
master_calidad <- master_calidad_temp %>%
  mutate(
    # Flag para 0 frentes totales
    detected_flags = ifelse(frentes_total == 0, 
                            ifelse(is.na(detected_flags), "66", paste(detected_flags, ",66", sep="")),
                            detected_flags),
    flag_trigger = ifelse(frentes_total == 0, -100, flag_trigger),
    # Flag para frentes insuficientes
    detected_flags = ifelse(frentes_total < min_frentes, 
                            ifelse(is.na(detected_flags), "61", paste(detected_flags, ",61", sep="")),
                            detected_flags),
    flag_trigger = ifelse(frentes_total < min_frentes, -100, flag_trigger),
    # Nueva flag para enfriador_total no nulo y frentes_total nulo
    detected_flags = ifelse(!is.na(enfriador_total) & is.na(frentes_total),
                            ifelse(is.na(detected_flags), "-100", paste(detected_flags, ",-100", sep="")),
                            detected_flags),
    flag_trigger = ifelse(!is.na(enfriador_total) & is.na(frentes_total), -100, flag_trigger)
  ) %>%
  select(-min_frentes)  # Eliminamos la columna temporal min_frentes

head(master_calidad)
```

## Evaluación de Data

### Union con Base de Datos de Parametros
Esta subsección del análisis se centra en enriquecer y evaluar la calidad del conjunto de datos maestro (`master_calidad`) a través de la incorporación de parámetros externos. Estos parámetros pueden incluir variables críticas que afectan la interpretación y el análisis posterior de los datos, tales como categorías de tamaño, canales comerciales y sub-canales específicos. La unión se realiza mediante campos clave que incluyen el tamaño del cliente, el código del canal comercial y el sub-canal.

```{r}
# Paso 1: Unión con parametros_data
master_evaluado <- master_calidad %>%
  left_join(parametros_data, by = c("tamaño_homologado" = "tamaño_homologado", "canal_isscom", "subcanal_isscom"))

# Paso 2: Generar subconjuntos basados en combinaciones de tradechannelcode, tamaño y sub_canal_isscom "General"
# Asegúrate de que cada subconjunto tenga exactamente una fila
tradicional_CH <- parametros_data[parametros_data$canal_isscom == "VIV.LOCALES TRADICIONALES" & 
                                  parametros_data$tamaño_homologado == "Chico" & 
                                  parametros_data$subcanal_isscom == "General", ][1, ]
tradicional_M  <- parametros_data[parametros_data$canal_isscom == "VIV.LOCALES TRADICIONALES" & 
                                  parametros_data$tamaño_homologado == "Mediano"  & 
                                  parametros_data$subcanal_isscom == "General", ][1, ]
tradicional_G  <- parametros_data[parametros_data$canal_isscom == "VIV.LOCALES TRADICIONALES" & 
                                  parametros_data$tamaño_homologado == "Grande"  & 
                                  parametros_data$subcanal_isscom == "General", ][1, ]

# Replicar los valores de cada subconjunto para que coincidan con el número de filas en master_evaluado
n_rows <- nrow(master_evaluado)
tradicional_CH_rep <- tradicional_CH[rep(1, n_rows), ]
tradicional_M_rep  <- tradicional_M[rep(1, n_rows), ]
tradicional_G_rep  <- tradicional_G[rep(1, n_rows), ]

# Paso 3: Usar bucle para iterar sobre columnas y reemplazar NAs
cols_to_modify <- c("lower_bound_frentes", "upper_bound_frentes", 
                    "lower_bound_frentes_arca", "upper_bound_frentes_arca", 
                    "lower_bound_enfriadores", "upper_bound_enfriadores", 
                    "lower_bound_duration", "upper_bound_duration",
                    "lower_bound_NumScenes", "upper_bound_NumScenes",
                    "lower_bound_ScenesAmb", "upper_bound_ScenesAmb",
                    "lower_bound_ScenesFrio", "upper_bound_ScenesFrio")

# Asegúrate de que cada subconjunto tenga al menos una fila de datos
if (nrow(tradicional_CH) == 0 || nrow(tradicional_M) == 0 || nrow(tradicional_G) == 0) {
  stop("Uno de los subconjuntos está vacío")
}

# Iterar sobre las columnas para reemplazar NAs
for(col in cols_to_modify) {
  # Tradicional CH
  master_evaluado[[col]][is.na(master_evaluado[[col]]) & 
                         master_evaluado$canal_isscom == "VIV.LOCALES TRADICIONALES" & 
                         master_evaluado$tamaño_homologado == "Chico"] <- tradicional_CH[[col]][1]
  
  # Tradicional M
  master_evaluado[[col]][is.na(master_evaluado[[col]]) & 
                         master_evaluado$canal_isscom == "VIV.LOCALES TRADICIONALES" & 
                         master_evaluado$tamaño_homologado == "Mediano"] <- tradicional_M[[col]][1]
  
  # Tradicional G
  master_evaluado[[col]][is.na(master_evaluado[[col]]) & 
                         master_evaluado$canal_isscom == "VIV.LOCALES TRADICIONALES" & 
                         master_evaluado$tamaño_homologado == "Grande"] <- tradicional_G[[col]][1]
  
  # NA en tradechannelcode
  master_evaluado[[col]][is.na(master_evaluado[[col]]) & 
                         is.na(master_evaluado$canal_isscom) & 
                         master_evaluado$tamaño_homologado == "Mediano"] <- tradicional_M[[col]][1]
  
  # Tamaño es NA pero tradechannelcode es "Tradicional"
  master_evaluado[[col]][is.na(master_evaluado[[col]]) & 
                         is.na(master_evaluado$tamaño_homologado) & 
                         master_evaluado$canal_isscom == "VIV.LOCALES TRADICIONALES"] <- tradicional_M[[col]][1]
}

# Paso 4
# Imputación final para reemplazar cualquier NA restante con los valores de 'tradicional_M'
for(col in cols_to_modify) {
  if(nrow(tradicional_M) > 0) {
    master_evaluado[[col]][is.na(master_evaluado[[col]])] <- tradicional_M[[col]][1]
  } else {
    stop("El subconjunto tradicional_M está vacío o no existe.")
  }
}
# Paso 5: Calificación
master_evaluado <- master_evaluado %>%
  mutate(
    # Calificación para frentes_total
    score_frentes_total = case_when(
      frentes_total >= lower_bound_frentes & frentes_total <= upper_bound_frentes ~ 100,
      frentes_total >= (lower_bound_frentes - 0.01 * (upper_bound_frentes - lower_bound_frentes)) &
        frentes_total <= (upper_bound_frentes + 0.01 * (upper_bound_frentes - lower_bound_frentes)) ~ 50,
      TRUE ~ 0
    ),
    # Calificación para frentes_total
    score_frentes_arca = case_when(
      frentes_arca >= lower_bound_frentes_arca & frentes_arca <= upper_bound_frentes_arca ~ 100,
      frentes_arca >= (lower_bound_frentes_arca - 0.01 * (upper_bound_frentes_arca - lower_bound_frentes_arca)) &
        frentes_arca <= (upper_bound_frentes_arca + 0.01 * (upper_bound_frentes_arca - lower_bound_frentes_arca)) ~ 50,
      TRUE ~ 0
    ),
    # Calificación para duration
    score_duration = case_when(
      duration >= lower_bound_duration & duration <= upper_bound_duration ~ 100,
      duration >= (lower_bound_duration - 0.05 * (upper_bound_duration - lower_bound_duration)) &
        duration <= (upper_bound_duration + 0.05 * (upper_bound_duration - lower_bound_duration)) ~ 50,
      TRUE ~ 0
    ),
    # Calificación para enfriador_total
    score_enfriador_total = case_when(
      enfriador_total >= lower_bound_enfriadores & enfriador_total <= upper_bound_enfriadores ~ 100,
      enfriador_total >= (lower_bound_enfriadores - 0.03 * (upper_bound_enfriadores - lower_bound_enfriadores)) &
        enfriador_total <= (upper_bound_enfriadores + 0.03 * (upper_bound_enfriadores - lower_bound_enfriadores)) ~ 50,
      TRUE ~ 0
    ),
    
    # Calificación para Num.Scenes
    score_NumScenes = case_when(
      Num.Scenes >= lower_bound_NumScenes & Num.Scenes <= upper_bound_NumScenes ~ 100,
      Num.Scenes >= (lower_bound_NumScenes - 0.1 * (upper_bound_NumScenes - lower_bound_NumScenes)) &
        Num.Scenes <= (upper_bound_NumScenes + 0.1 * (upper_bound_NumScenes - lower_bound_NumScenes)) ~ 50,
      TRUE ~ 0
    ),
    # Calificación para Scenes_Amb
    score_ScenesAmb = case_when(
      Scenes_Amb >= lower_bound_ScenesAmb & Scenes_Amb <= upper_bound_ScenesAmb ~ 100,
      Scenes_Amb >= (lower_bound_ScenesAmb - 0.1 * (upper_bound_ScenesAmb - lower_bound_ScenesAmb)) &
        Scenes_Amb <= (upper_bound_ScenesAmb + 0.1 * (upper_bound_ScenesAmb - lower_bound_ScenesAmb)) ~ 50,
      TRUE ~ 0
    ),
    # Calificación para Scenes_Frio
    score_ScenesFrio = case_when(
      Scenes_Frio >= lower_bound_ScenesFrio & Scenes_Frio <= upper_bound_ScenesFrio ~ 100,
      Scenes_Frio >= (lower_bound_ScenesFrio - 0.1 * (upper_bound_ScenesFrio - lower_bound_ScenesFrio)) &
        Scenes_Frio <= (upper_bound_ScenesFrio + 0.1 * (upper_bound_ScenesFrio - lower_bound_ScenesFrio)) ~ 50,
      TRUE ~ 0
    )
  )

# Verificar el resultado
head(master_evaluado)
```

### Imputación de Territorio según coordenadas
En esta parte del proceso, nos aseguramos de que los datos geográficos estén disponibles en el conjunto de datos evaluado. Al integrar las coordenadas de latitud y longitud de las sesiones, podemos realizar análisis geográficos o imputaciones de territorio basadas en estas coordenadas. Esto es esencial para cualquier análisis que requiera una dimensión espacial o para validar la coherencia geográfica de los datos.

```{r message=FALSE, warning=FALSE}
# Asegurándonos que SessionUID es del mismo tipo en ambos dataframes
df_session$SessionUID <- as.character(df_session$SessionUId)
master_evaluado$SessionUID <- as.character(master_evaluado$SessionUID)

# Uniendo las coordenadas de df_session a master_calidad
master_evaluado <- master_evaluado %>%
  left_join(df_session %>% select(SessionUID, latitude, longitude),
            by = "SessionUID")
```

## Guardado de Data

### Almacenamiento Mensual
Esta parte del proceso involucra la organización y el almacenamiento de los datos evaluados en archivos separados por mes y año. Esto permite una fácil recuperación y análisis de los datos basados en intervalos de tiempo específicos, manteniendo un control adecuado sobre la evolución de los datos y facilitando el acceso histórico cuando sea necesario.

```{r message=FALSE, warning=FALSE}
# Asegurarse de que 'Survey End Time' está en formato de fecha
master_evaluado$`Survey End Time` <- as.Date(master_evaluado$`Survey End Time`, format = "%Y-%m-%d")

# Convertir la columna 'detected_flags' a tipo caracter para evitar problemas al combinar dataframes
master_evaluado$detected_flags <- as.character(master_evaluado$detected_flags)

# Convertir 'Outlet Code' a tipo caracter en master_evaluado para evitar problemas al combinar
master_evaluado$`Outlet Code` <- as.character(master_evaluado$`Outlet Code`)

# Extraer mes y año de 'Survey End Time' para organizar los archivos
master_evaluado$mes_año <- format(master_evaluado$`Survey End Time`, "%Y-%m")

# Dividir los datos por mes y año, y guardar cada subconjunto en un archivo CSV correspondiente
unique_meses <- unique(master_evaluado$mes_año)

for(mes in unique_meses) {
  subset_datos <- filter(master_evaluado, mes_año == mes)
  
  # Convertir 'detected_flags' en subset_datos a caracter para mantener consistencia
  subset_datos$detected_flags <- as.character(subset_datos$detected_flags)
  
  # Crear el nombre del archivo basado en el mes y año
  archivo_nombre <- paste0("data_procesada/", mes, ".csv")
  
  # Si ya existe el archivo, leerlo, convertir 'detected_flags' a caracter y combinarlo con los datos nuevos antes de escribir de nuevo
  if (file.exists(archivo_nombre)) {
    datos_existentes <- read_csv(archivo_nombre)
    datos_existentes$detected_flags <- as.character(datos_existentes$detected_flags)
    datos_existentes$`Outlet Code` <- as.character(datos_existentes$`Outlet Code`)
    subset_datos <- bind_rows(datos_existentes, subset_datos) %>%
      distinct()  # Asegurar que no hay duplicados
  }
  
  # Guardar el subconjunto de datos en el archivo
  write_csv(subset_datos, archivo_nombre)
}

cat("Los datos han sido actualizados y guardados en archivos separados por mes y año en 'data_procesada/'.\n")
```


## Limpieza de Data Procesada

### Duplicados e Incompletos
En esta etapa del procesamiento, se lleva a cabo una limpieza exhaustiva de los datos almacenados mensualmente. Se enfoca en identificar y eliminar registros duplicados, además de mejorar registros incompletos donde la presencia de valores NA y ceros pueda ser indicativa de datos faltantes o incorrectos. Este proceso asegura que cada `SessionUID` esté representado por la entrada más completa disponible.

```{r message=FALSE, warning=FALSE}
# Definir la carpeta donde se encuentran los datos
folder_path <- "data_procesada"

# Leer todos los archivos CSV en la carpeta
files <- list.files(path = folder_path, pattern = "*.csv", full.names = TRUE)

# Función para procesar cada archivo
process_file <- function(file_path) {
  # Leer el archivo
  data <- read_csv(file_path)

  # Calcular la cantidad de NAs y 0s por fila
  data <- data %>%
    mutate(cantidad_na = rowSums(is.na(.)),
           cantidad_ceros = rowSums(. == 0, na.rm = TRUE))

  # Combinar las cantidades de NAs y 0s para tener un criterio de selección
  data <- data %>%
    mutate(cantidad_na_ceros = cantidad_na + cantidad_ceros)

  # Seleccionar la entrada con menos NA y 0 para cada SessionUID y eliminar los contadores
  data <- data %>%
    group_by(SessionUID) %>%
    arrange(cantidad_na_ceros) %>%
    slice(1) %>%
    ungroup() %>%
    select(-c(cantidad_na, cantidad_ceros, cantidad_na_ceros))

  # Guardar el archivo procesado
  write_csv(data, file_path)
}

# Aplicar la función a cada archivo
walk(files, process_file)

# Mensaje de confirmación
cat("Todos los archivos en la carpeta", folder_path, "han sido procesados y limpiados considerando valores NA y 0.\n")
```

## Sabana de Calidad Madre

### Maestro de Data Procesada
Esta parte del proceso implica la consolidación de todos los archivos CSV individuales que han sido previamente procesados y limpiados. El objetivo es combinar estos archivos en un único dataframe maestro que facilitará el análisis agregado y más complejo. Durante este proceso, nos aseguramos de que todos los tipos de datos sean consistentes y que no haya conflictos en el formato.

```{r message=FALSE, warning=FALSE}
# Ruta a la carpeta con los archivos procesados
ruta_carpeta <- "data_procesada/"

# Obtener lista de todos los archivos CSV en la carpeta
archivos <- dir_ls(ruta_carpeta, regexp = "\\.csv$")

# Función para leer un archivo CSV y asegurarse de que todas las columnas sean del mismo tipo
leer_y_convertir <- function(archivo) {
  df <- read_csv(archivo)
  # Convertir todas las columnas a character para evitar conflictos
  df[] <- lapply(df, as.character)
  return(df)
}

# Leer cada archivo, convertir columnas y combinarlos en un solo dataframe
master_calidad <- map_dfr(archivos, leer_y_convertir)

# Asumiendo que master_calidad es tu dataframe
master_calidad <- master_calidad %>%
  mutate(
    duration = as.numeric(duration),
    distance = as.numeric(distance),
    frentes_total = as.numeric(frentes_total),
    frentes_arca = as.numeric(frentes_arca),
    sovi = as.numeric(sovi),
    flag_trigger = as.numeric(flag_trigger),
    enfriador_total = as.numeric(enfriador_total),
    score_frentes_total = as.numeric(score_frentes_total),
    score_frentes_arca = as.numeric(score_frentes_arca),
    score_duration = as.numeric(score_duration),
    score_enfriador_total = as.numeric(score_enfriador_total),
    score_ScenesAmb = as.numeric(score_ScenesAmb),
    score_ScenesFrio = as.numeric(score_ScenesFrio),
    score_NumScenes = as.numeric(score_NumScenes),
    latitude = as.numeric(latitude),
    longitude = as.numeric(longitude),
    estatus = as.numeric(estatus),
    lower_bound_frentes = as.numeric(lower_bound_frentes),
    upper_bound_frentes = as.numeric(upper_bound_frentes),
    lower_bound_frentes_arca = as.numeric(lower_bound_frentes_arca),
    upper_bound_frentes_arca = as.numeric(upper_bound_frentes_arca),
    lower_bound_enfriadores = as.numeric(lower_bound_enfriadores),
    upper_bound_enfriadores = as.numeric(upper_bound_enfriadores),
    lower_bound_duration = as.numeric(lower_bound_duration),
    upper_bound_duration = as.numeric(upper_bound_duration),
    lower_bound_ScenesAmb = as.numeric(lower_bound_ScenesAmb),
    upper_bound_ScenesAmb = as.numeric(upper_bound_ScenesAmb),
    lower_bound_ScenesFrio = as.numeric(lower_bound_ScenesFrio),
    upper_bound_ScenesFrio = as.numeric(upper_bound_ScenesFrio),
    lower_bound_NumScenes = as.numeric(lower_bound_NumScenes),
    upper_bound_NumScenes = as.numeric(upper_bound_NumScenes),
  )

# Guardar el dataframe combinado como un archivo Excel
write.xlsx(master_calidad, "master_calidad.xlsx")

# Mensaje de confirmación
cat("El archivo 'master_calidad.xlsx' ha sido guardado con éxito.\n")
```

### Sabana de ICD Maestra
Este proceso implica cargar, evaluar y almacenar la data procesada, realizando cálculos avanzados para determinar la calidad integral de los datos (ICD) basándose en múltiples parámetros y ponderaciones. La data evaluada se ajusta luego para asegurar que cumpla con criterios específicos antes de ser guardada para uso futuro.

```{r}
# Path a data procesada
path_data_procesada <- "master_calidad.xlsx"

# Cargar la data procesada
data_procesada <- read_xlsx(path_data_procesada)

# Crear una carpeta para la data evaluada si no existe
path_data_evaluada <- "data_evaluada"
dir_create(path_data_evaluada)

# Parámetros con sus ponderaciones
ponderacion_tiempo <- 0.10
ponderacion_escenas_totales <- 0.10
ponderacion_escenas_ambiente <- 0.05
ponderacion_escenas_frio <- 0.05
ponderacion_coordenadas <- 0.05
ponderacion_flags <- 0.18
ponderacion_enfriadores <- 0.05
ponderacion_frentes_arca <- 0.15
ponderacion_frentes <- 0.06
ponderacion_sovi <- 0.24

# Funciones auxiliares para cálculos específicos
calcular_calificacion_coordenadas <- function(distancia) {
  if (is.na(distancia)) return(0)
  if (distancia <= 75) return(100)
  max(50 - 10 * floor((distancia - 75 - 100) / 100), 0)
}

calcular_calificacion_sovi <- function(tradechannelcode, sovi) {
  if (is.na(sovi)) {
    return(0)
  }
  if (sovi <= 0.75) {
    return(100)
  }
  return(max(100 - 10 * floor((sovi - 0.75) / 0.03), 0))
}

# Aplicar las funciones auxiliares a las columnas relevantes
data_procesada <- data_procesada %>%
  mutate(
    calificacion_coordenadas = map_dbl(distance, calcular_calificacion_coordenadas),
    calificacion_sovi = pmap_dbl(list(canal_isscom, sovi), calcular_calificacion_sovi)
  )

# Calcular ICD
data_procesada <- data_procesada %>%
  mutate(
    # Calcular ICD considerando NA como 0 solo en la suma final por variable
    ICD = pmax(0,
               (if_else(is.na(score_duration), 0, score_duration) * ponderacion_tiempo) +
               (if_else(is.na(score_NumScenes), 0, score_NumScenes) * ponderacion_escenas_totales) +
               (if_else(is.na(score_ScenesAmb), 0, score_ScenesAmb) * ponderacion_escenas_ambiente) +
               (if_else(is.na(score_ScenesFrio), 0, score_ScenesFrio) * ponderacion_escenas_frio) +
               (if_else(is.na(calificacion_coordenadas), 0, calificacion_coordenadas) * ponderacion_coordenadas) +
               (if_else(is.na(flag_trigger), 0, flag_trigger) * ponderacion_flags) +
               (if_else(is.na(score_enfriador_total), 0, score_enfriador_total) * ponderacion_enfriadores) +
               (if_else(is.na(score_frentes_arca), 0, score_frentes_arca) * ponderacion_frentes_arca) +
               (if_else(is.na(score_frentes_total), 0, score_frentes_total) * ponderacion_frentes) +
               (if_else(is.na(calificacion_sovi), 0, calificacion_sovi) * ponderacion_sovi)
    ),
    ICD = round(ICD, 1)  # Redondeo a 1 decimal
  )

# Asegurarse de que Survey End Time es de tipo fecha
data_procesada$`Survey End Time` <- as.Date(data_procesada$`Survey End Time`)

# Calcular la diferencia en días desde la fecha de la encuesta hasta hoy
data_procesada <- data_procesada %>%
  mutate(
    dias_desde_encuesta = as.numeric(Sys.Date() - `Survey End Time`)
  )

# Calcular Aprobación con la nueva condición
data_procesada <- data_procesada %>%
  mutate(
    dias_desde_encuesta = as.numeric(Sys.Date() - as.Date(`Survey End Time`, format = "%Y-%m-%d")), # Asegurar que `Survey End Time` se trate como fecha
    NA_count = rowSums(is.na(select(., duration, distance, frentes_total, frentes_arca, sovi, enfriador_total, Num.Scenes, Scenes_Amb, Scenes_Frio))),
    Aprobacion = case_when(
      flag_trigger == -100 & ICD < 70 ~ "Reprobado",
      flag_trigger == -100 ~ "Aprobado",
      NA_count > 4 & dias_desde_encuesta <= 14 ~ "DF",  # Usa dias_desde_encuesta para aplicar la regla de DF solo si es dentro de los últimos 14 días
      ICD < 70 ~ "Reprobado",
      TRUE ~ "Aprobado"
    )
  ) %>%
  select(-NA_count, -dias_desde_encuesta)  # Eliminar columnas temporales después de su uso

# Calcular Notas
data_procesada <- data_procesada %>%
  mutate(
    Notas = case_when(
      Aprobacion == "DF" ~ "En Procesamiento",
      Aprobacion == "Aprobado" ~ "",
      TRUE ~ paste0(
        if_else(score_duration <= 50, "Tiempo fuera de rango, ", ""),
        if_else(calificacion_coordenadas <= 50, "Coordenadas: lejos de PDV, ", ""),
        if_else(flag_trigger != 100, "Problemas con las flags, ", ""),
        if_else(is.na(detected_flags), "",  # Evita agregar texto si detected_flags es NA
                paste0(
                  if_else(str_detect(detected_flags, "13"), "Foto de foto, ", ""),
                  if_else(str_detect(detected_flags, "14"), "Tipo de escena incorrecto, ", ""),
                  if_else(str_detect(detected_flags, "15"), "Mal ángulo, ", ""),
                  if_else(str_detect(detected_flags, "16"), "Imagen borrosa, ", ""),
                  if_else(str_detect(detected_flags, "2"), "Parte del cuerpo obstruyendo, ", ""),
                  if_else(str_detect(detected_flags, "4"), "Muy oscuro, ", ""),
                  if_else(str_detect(detected_flags, "26"), "Múltiples POCs desconectados en una escena, ", ""),
                  if_else(str_detect(detected_flags, "88"), "Frentes Insuficientes, ", ""),
                  if_else(str_detect(detected_flags, "27"), "Puerta cerrada, ", ""),
                  if_else(str_detect(detected_flags, "32"), "Imagen de objetos (No enfriadores ni frentes), ", ""),
                  if_else(str_detect(detected_flags, "61"), "Foto sin Frentes, ", ""),
                  if_else(str_detect(detected_flags, "36"), "Imagen duplicada, ", ""),
                  if_else(str_detect(detected_flags, "39"), "Objetos obstaculizando, ", ""),
                  if_else(str_detect(detected_flags, "66"), "Sesión con 0 productos (frentes) en imágenes, ", "")
                )),
        if_else(score_enfriador_total <= 50, "Enfriadores en PDV no coincide con Comodato, ", ""),
        if_else(score_NumScenes <= 50, "Número de fotografías fuera de rango, ", ""),
        if_else(score_ScenesAmb <= 50, "Número de fotos de ambiente fuera de cantidad estándar, ", ""),
        if_else(score_ScenesFrio <= 50, "Número de fotos de frío fuera de cantidad estándar, ", ""),
        if_else(score_frentes_total <= 50, "Cantidad de frentes fuera de rango, ", ""),
        if_else(score_frentes_arca <= 50, "Falta fotografiar competencia", ""),
        sep = ""
      )
    )
  )

# Aprobar automáticamente si es un "AUD"
data_procesada <- data_procesada %>%
  mutate(
    Aprobacion = ifelse(substr(User, 1, 3) == "AUD", "Aprobado", Aprobacion),
    Notas = ifelse(substr(User, 1, 3) == "AUD", "Aprobado automáticamente por ser auditoría", Notas)
  )

# Seleccion de Variables
data_evaluada <- data_procesada %>%
  select(
    SessionUID, SurveyType, User, `Outlet Code`, region_comercial, tamaño_homologado, canal_isscom, 
    subcanal_isscom, duration, distance, frentes_total, frentes_arca, sovi, flag_trigger, 
    enfriador_total, Num.Scenes, Scenes_Amb, Scenes_Frio, detected_flags, 
    subregion_comercial, territorio, 
    `Survey End Time`, ICD, estatus, Aprobacion, Notas
  )

# Guardar los resultados
write_csv(data_evaluada, file.path(path_data_evaluada, "peru_icd3.csv"))
```


#Extracto Clientes
```{r}
extracto_clientes <- master_clientes %>%
  select(
    tradechannelcode, sub_canal_isscom, salesgroupcode, salesorganizationcode,
    country, subclientcode, tamaño, ruta_preventa_oficial, customercode, customername, 
    modelo_de_servicio_ruta, isactive, zona...13, `territorio...14`, salesterritorycode
  )

# Guardar el dataframe en un archivo Excel
write.xlsx(extracto_clientes, "extracto_clientes.xlsx")

# Si quieres confirmar que el archivo se ha guardado
cat("Archivo 'extracto_clientes.xlsx' guardado con éxito.")
```

------- BUSQUEDA ----------

### Actual
```{r}
result_dplyr <- df_actual %>%
  filter(SessionUID == "d5d287fa-89cf-4e4c-88ef-97dfbb46bc1a")

# Mostrando el resultado
print(result_dplyr)
```

```{r}
summary(master_evaluado)
```


---------- DIVISION DE DATA POR MES -------------
```{r message=FALSE, warning=FALSE}
# Ruta al archivo Excel
archivo_excel <- "ICD - Indice de Calidad de Data.xlsx"

# Leer los datos de la pestaña "Data"
df <- read_excel(archivo_excel, sheet = "Data")

# Convertir la columna "Survey End Time" a tipo fecha
df$Survey_End_Time <- as.Date(df$`Survey End Time`, format = "%m/%d/%Y")

# Crear una columna con el mes y año
df$Mes_Año <- format(df$Survey_End_Time, "%B_%Y")

# Crear una carpeta para almacenar los datos procesados si no existe
ruta_carpeta <- "data_procesada"
if (!dir.exists(ruta_carpeta)) {
  dir.create(ruta_carpeta, recursive = TRUE)
  
  # Verificar si la carpeta fue creada
  if (!dir.exists(ruta_carpeta)) {
    stop("La carpeta no pudo ser creada. Verifica los permisos.")
  }
}

# Función para guardar los archivos CSV por mes
guardar_csv_por_mes <- function(data, mes, ruta_carpeta) {
  ruta_archivo <- file.path(ruta_carpeta, paste0(mes, ".csv"))
  write.csv(data, ruta_archivo, row.names = FALSE)
  cat("Archivo guardado:", ruta_archivo, "\n")
}

# Aplicar la función a cada mes
lapply(unique(df$Mes_Año), function(mes) {
  datos_mes <- filter(df, Mes_Año == mes)
  if(nrow(datos_mes) > 0) {
    guardar_csv_por_mes(datos_mes, mes, ruta_carpeta)
  } else {
    cat("No hay datos para el mes:", mes, "\n")
  }
})

cat("Todos los archivos han sido creados.\n")
```

-------- QUERIES PERSONALIZADOS --------
```{r}
# Asegúrate de que 'Survey End Time' es de tipo Date
master_evaluado$`Survey End Time` <- as.Date(master_evaluado$`Survey End Time`, format = "%Y-%m-%d")

# Crear una nueva columna con la fecha del lunes de la semana correspondiente
master_evaluado <- master_evaluado %>%
  mutate(
    start_of_week = floor_date(`Survey End Time`, unit = "week"),
    week_label = format(start_of_week, "%d %b"),
    week_order = as.numeric(start_of_week)
  )

# Agrupar por 'salesterritorycode' y 'week_label', y contar 'SessionUID' únicos
conteo_semanal <- master_evaluado %>%
  group_by(salesterritorycode, week_label) %>%
  summarise(total_count = n_distinct(SessionUID), .groups = 'drop') %>%
  ungroup()

# Ordenar el dataframe por 'salesterritorycode' y 'week_order'
# Nota: Es importante usar 'week_order' solo para ordenar antes del pivot_wider y no incluirlo en el pivot_wider para evitar duplicados.
conteo_semanal <- conteo_semanal %>%
  arrange(salesterritorycode, week_order)

# Pivotar los datos para tener las etiquetas de semanas como columnas y 'salesterritorycode' como filas
tabla_conteo_semanal <- conteo_semanal %>%
  pivot_wider(
    names_from = week_label,
    values_from = total_count,
    values_fill = list(total_count = 0)
  )

# Ordenar las columnas por fecha usando 'week_order' de 'master_evaluado'
column_order <- unique(master_evaluado$week_label[order(master_evaluado$week_order)])
tabla_conteo_semanal <- tabla_conteo_semanal %>%
  select(salesterritorycode, all_of(column_order))

# Guardar el dataframe en un archivo Excel
write.xlsx(tabla_conteo_semanal, "/mnt/data/tabla_conteo_semanal.xlsx")
```


```{r}
# Asegurarse de que las variables están en el formato correcto
master_evaluado$frentes_total <- as.numeric(master_evaluado$frentes_total)

# Ordenar los niveles de 'tamaño_homologado'
master_evaluado$tamaño_homologado <- factor(master_evaluado$tamaño_homologado, 
                                            levels = c("Chico", "Mediano", "Grande", NA))

# Crear un gráfico de caja para mostrar la dispersión de 'frentes_total' agrupado por 'tamaño_homologado'
ggplot(master_evaluado, aes(x = tamaño_homologado, y = frentes_total)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Dispersión de la variable 'frentes_total' por 'tamaño_homologado'",
       x = "Tamaño Homologado",
       y = "Frentes Total")
```

